{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master=\"yarn\"\n",
    "launcher.num_executors=6\n",
    "launcher.executor_cores=2\n",
    "launcher.executor_memory='2000m'\n",
    "launcher.packages=[\"com.github.master:spark-stemming_2.10:0.2.0\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://bd-hm04:8088/proxy/application_1544314460278_0001\n",
       "SparkContext available as 'sc' (version = 2.3.2, master = yarn, app id = application_1544314460278_0001)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-08 18:16:29 WARN  Utils:66 - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "root\n",
      " |-- Time: decimal(10,0) (nullable = true)\n",
      " |-- V1: double (nullable = true)\n",
      " |-- V2: double (nullable = true)\n",
      " |-- V3: double (nullable = true)\n",
      " |-- V4: double (nullable = true)\n",
      " |-- V5: double (nullable = true)\n",
      " |-- V6: double (nullable = true)\n",
      " |-- V7: double (nullable = true)\n",
      " |-- V8: double (nullable = true)\n",
      " |-- V9: double (nullable = true)\n",
      " |-- V10: double (nullable = true)\n",
      " |-- V11: double (nullable = true)\n",
      " |-- V12: double (nullable = true)\n",
      " |-- V13: double (nullable = true)\n",
      " |-- V14: double (nullable = true)\n",
      " |-- V15: double (nullable = true)\n",
      " |-- V16: double (nullable = true)\n",
      " |-- V17: double (nullable = true)\n",
      " |-- V18: double (nullable = true)\n",
      " |-- V19: double (nullable = true)\n",
      " |-- V20: double (nullable = true)\n",
      " |-- V21: double (nullable = true)\n",
      " |-- V22: double (nullable = true)\n",
      " |-- V23: double (nullable = true)\n",
      " |-- V24: double (nullable = true)\n",
      " |-- V25: double (nullable = true)\n",
      " |-- V26: double (nullable = true)\n",
      " |-- V27: double (nullable = true)\n",
      " |-- V28: double (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      "\n",
      "+----+-----------------+-------------------+----------------+-----------------+------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+------+-----+\n",
      "|Time|               V1|                 V2|              V3|               V4|                V5|                 V6|                 V7|                V8|                V9|               V10|               V11|               V12|               V13|               V14|              V15|               V16|               V17|               V18|               V19|                V20|               V21|               V22|               V23|               V24|               V25|               V26|                 V27|                V28|Amount|Class|\n",
      "+----+-----------------+-------------------+----------------+-----------------+------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+------+-----+\n",
      "|   0| -1.3598071336738|-0.0727811733098497|2.53634673796914| 1.37815522427443|-0.338320769942518|  0.462387777762292|  0.239598554061257|0.0986979012610507| 0.363786969611213|0.0907941719789316|-0.551599533260813|-0.617800855762348|-0.991389847235408|-0.311169353699879| 1.46817697209427|-0.470400525259478| 0.207971241929242|0.0257905801985591| 0.403992960255733|  0.251412098239705|-0.018306777944153| 0.277837575558899|-0.110473910188767|0.0669280749146731| 0.128539358273528|-0.189114843888824|   0.133558376740387|-0.0210530534538215|149.62|    0|\n",
      "|   0| 1.19185711131486|   0.26615071205963|0.16648011335321|0.448154078460911|0.0600176492822243|-0.0823608088155687|-0.0788029833323113|0.0851016549148104|-0.255425128109186|-0.166974414004614|  1.61272666105479|  1.06523531137287|  0.48909501589608|-0.143772296441519|0.635558093258208| 0.463917041022171|-0.114804663102346|-0.183361270123994|-0.145783041325259|-0.0690831352230203|-0.225775248033138|-0.638671952771851| 0.101288021253234|-0.339846475529127| 0.167170404418143| 0.125894532368176|-0.00898309914322813| 0.0147241691924927|  2.69|    0|\n",
      "|   1|-1.35835406159823|  -1.34016307473609|1.77320934263119|0.379779593034328|-0.503198133318193|   1.80049938079263|  0.791460956450422| 0.247675786588991| -1.51465432260583| 0.207642865216696| 0.624501459424895| 0.066083685268831| 0.717292731410831|-0.165945922763554| 2.34586494901581| -2.89008319444231|  1.10996937869599|-0.121359313195888| -2.26185709530414|  0.524979725224404| 0.247998153469754| 0.771679401917229| 0.909412262347719|-0.689280956490685|-0.327641833735251|-0.139096571514147| -0.0553527940384261|-0.0597518405929204|378.66|    0|\n",
      "+----+-----------------+-------------------+----------------+-----------------+------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "creditCard_df: org.apache.spark.sql.DataFrame = [Time: decimal(10,0), V1: double ... 29 more fields]\n",
       "res0: Long = 284807\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Read the CSV file and load it into a dataframe.the \"inferschema\" parameter is set to true\n",
    "val creditCard_df=spark.read.option(\"header\",\"true\").option(\"inferschema\", \"true\").csv(\"/hadoop-user/data/creditcard.csv\")\n",
    "creditCard_df.cache()\n",
    "creditCard_df.printSchema()\n",
    "creditCard_df.show(3)\n",
    "creditCard_df.count\n",
    "//creditCard_df.createOrReplaceTempView(\"creditCard_df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res69: Long = 284807\n"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// to check for null values in our dataset\n",
    "creditCard_df.na.drop\n",
    "creditCard_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//get idea of mean, mode,st deviation\n",
    "creditCard_df.describe().show(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|creditCard_df|\n",
      "+-------------+\n",
      "| [149.62,0.0]|\n",
      "|   [2.69,0.0]|\n",
      "| [378.66,1.0]|\n",
      "|  [123.5,1.0]|\n",
      "|  [69.99,2.0]|\n",
      "|   [3.67,2.0]|\n",
      "|   [4.99,4.0]|\n",
      "|   [40.8,7.0]|\n",
      "|   [93.2,7.0]|\n",
      "|   [3.68,9.0]|\n",
      "|   [7.8,10.0]|\n",
      "|  [9.99,10.0]|\n",
      "| [121.5,10.0]|\n",
      "|  [27.5,11.0]|\n",
      "|  [58.8,12.0]|\n",
      "| [15.99,12.0]|\n",
      "| [12.99,12.0]|\n",
      "|  [0.89,13.0]|\n",
      "|  [46.8,14.0]|\n",
      "|   [5.0,15.0]|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature._\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_b62ec09eb19e\n",
       "feature_vector: Unit = ()\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature._\n",
    "//vector assembler to assemble these columns to a single feature vector.\n",
    "val assembler=new VectorAssembler().setInputCols(Array(\"Amount\",\"Time\")).setOutputCol(\"feature_vector\")\n",
    "\n",
    "//val feature_vector=assembler.transform(creditCard_df).select(\"feature_vector\").toDF(\"creditCard_df\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdScal_e5d9a5664efc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature._\n",
       "import org.apache.spark.ml.linalg._\n",
       "standardized: org.apache.spark.ml.feature.StandardScaler = stdScal_e5d9a5664efc\n"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.ml.linalg._\n",
    "\n",
    "val standardized=new StandardScaler().setWithMean(true).setInputCol(\"feature_vector\").setOutputCol(\"scaled_and_centered\")\n",
    "println(standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler1: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_0dbb091556d8\n"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// another vector assembler to assemble remaining columns to a single feature vector.\n",
    "val assembler1=new VectorAssembler().setInputCols(Array(\"scaled_and_centered\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\",\"V7\",\"V8\",\"V9\",\"V10\",\"V11\",\"V12\",\"V13\",\"V14\",\"V15\",\"V16\",\"V17\",\"V18\",\"V19\",\"V20\",\"V21\",\"V22\",\"V23\",\"V24\",\"V25\",\"V26\",\"V27\",\"V28\")).setOutputCol(\"feature_vector1\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minmax: org.apache.spark.ml.feature.MinMaxScaler = minMaxScal_0df5b5e00432\n"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//MinMaxScaler estimator which takes a column of feature vectors and computes its min-max scaling\n",
    "val minmax=new MinMaxScaler().setInputCol(\"feature_vector1\").setOutputCol(\"min_max_scaled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num: Long = 492\n",
       "num1: Long = 284315\n",
       "total_count: Long = 284807\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Getting count of fraud and non-fraud class\n",
    "val num=sql(\"select Class from creditCard_df where Class= 1\").count()\n",
    "val num1=sql(\"select Class from creditCard_df where Class= 0\").count()\n",
    "val total_count=creditCard_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9982725143693799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "balance_ratio: Double = 0.9982725143693799\n",
       "calculateWeight: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,DoubleType,Some(List(DoubleType)))\n",
       "weightedDataset: org.apache.spark.sql.DataFrame = [Time: decimal(10,0), V1: double ... 30 more fields]\n"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// using weighted data(under-sampling) to balance the data\n",
    "val balance_ratio=(total_count-num).toDouble / total_count\n",
    "println(balance_ratio)\n",
    "val calculateWeight= udf { d: Double =>\n",
    "if (d == 0.0) {\n",
    "    1 * balance_ratio\n",
    "}\n",
    "    else{\n",
    "        (1 * (1.0 - balance_ratio))\n",
    "    }\n",
    "}\n",
    "// cresting new dataset with balanced data\n",
    "val weightedDataset = creditCard_df.withColumn(\"classWeightCol\", calculateWeight(creditCard_df(\"Class\")))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr_w: org.apache.spark.ml.classification.LogisticRegression = logreg_b826b840105c\n"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//create a logistic regression model using LogisticRegression class in spark and set its input column to the classWeightCol and its output column to the user defined Class\n",
    "val lr_w= new LogisticRegression().setWeightCol(\"classWeightCol\").setLabelCol(\"Class\").setFeaturesCol(\"feature_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_w: org.apache.spark.ml.Pipeline = pipeline_99719d8736fb\n"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline_w = new Pipeline().setStages(Array(assembler,standardized,lr_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training_w: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Time: decimal(10,0), V1: double ... 30 more fields]\n",
       "testing_w: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Time: decimal(10,0), V1: double ... 30 more fields]\n"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training_w,testing_w)=weightedDataset.randomSplit(Array(0.8,0.2),111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipelineModel_w: org.apache.spark.ml.PipelineModel = pipeline_99719d8736fb\n"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Fit the training data to the pipeline\n",
    "val pipelineModel_w = pipeline_w.fit(training_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions_w: org.apache.spark.sql.DataFrame = [Time: decimal(10,0), V1: double ... 35 more fields]\n"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Make predictions.\n",
    "val predictions_w = pipelineModel_w.transform(testing_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|Class|         probability|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|    0|[0.99999446344923...|\n",
      "|       0.0|    0|[0.99999433352264...|\n",
      "|       0.0|    0|[0.99999443769908...|\n",
      "|       0.0|    0|[0.99999440327654...|\n",
      "|       0.0|    0|[0.99999444317901...|\n",
      "|       0.0|    0|[0.99999446725021...|\n",
      "|       0.0|    0|[0.99999442235565...|\n",
      "|       0.0|    0|[0.99999446644778...|\n",
      "|       0.0|    0|[0.99999445119999...|\n",
      "|       0.0|    0|[0.99999444660485...|\n",
      "|       0.0|    0|[0.99999270452405...|\n",
      "|       0.0|    0|[0.99999444503214...|\n",
      "|       0.0|    0|[0.99999433526159...|\n",
      "|       0.0|    0|[0.99999443338983...|\n",
      "|       0.0|    0|[0.99999440339310...|\n",
      "|       0.0|    0|[0.99999431936653...|\n",
      "|       0.0|    0|[0.99999444741057...|\n",
      "|       0.0|    0|[0.99999446158170...|\n",
      "|       0.0|    0|[0.99999446918852...|\n",
      "|       0.0|    0|[0.99999445427104...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Area under ROC curve(AUC) for RF on test data = 0.5490358212872164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "paramGrid_w: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlogreg_b826b840105c-elasticNetParam: 0.0,\n",
       "\tlogreg_b826b840105c-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_b826b840105c-elasticNetParam: 0.5,\n",
       "\tlogreg_b826b840105c-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_b826b840105c-elasticNetParam: 1.0,\n",
       "\tlogreg_b826b840105c-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_b826b840105c-elasticNetParam: 0.0,\n",
       "\tlogreg_b826b840105c-regParam: 0.5\n",
       "}, {\n",
       "\tlogreg_b826b840105c-elasticNetParam: 0.5,\n",
       "\tlogreg_b826b840105c-regParam: 0.5\n",
       "}, {\n",
       "\tlogreg_b826b840105c-elasticNetParam: 1.0,\n",
       "\tlogreg_b826b840105c-regParam: 0.5\n",
       "}, {\n",
       "\tlogreg_b826b840105c-elasticNetParam: 0.0,\n",
       "\tlogreg_b826b840105c-regParam: 2.0\n",
       "}, {\n",
       "\tlogreg_b826b840105c-elasticNetParam: 0.5,\n",
       "\tlogreg_b826b840105c-regParam: 2.0\n",
       "}, {\n",
       "\tlogreg_b826b840105c-elasticNetParam: 1.0,\n",
       "\tlogreg_b826b840105c-r..."
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paramGrid_w =new ParamGridBuilder()\n",
    "             .addGrid(lr_w.regParam, Array(0.01, 0.5, 2.0))\n",
    "             .addGrid(lr_w.elasticNetParam, Array(0.0, 0.5, 1.0))\n",
    "             .build()\n",
    "//evaluate the model with the test data.\n",
    "val evaluator_w = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"Class\").setMetricName(\"areaUnderROC\")\n",
    "//a 5 fold cross validation is used to tune hyper-parameters\n",
    "val cv_w = new CrossValidator().setEstimator(lr_w).setEvaluator(evaluator_w).setEstimatorParamMaps(paramGrid_w).setNumFolds(5)\n",
    "\n",
    "\n",
    "//// Select example rows to display.\n",
    "predictions_w.select(\"prediction\",\"Class\",\"probability\").show()\n",
    "val AUC_w = evaluator_w.evaluate(predictions_w)\n",
    "println(s\"Area under ROC curve(AUC) for RF on test data = $AUC_w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|Class| count|\n",
      "+-----+------+\n",
      "|    1|   492|\n",
      "|    0|284315|\n",
      "+-----+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count: Unit = ()\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//displaying count of target variable\n",
    "val count = creditCard_df.groupBy(\"Class\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fractionKeyMap1: scala.collection.immutable.Map[Double,Double] = Map(1.0 -> 1.0, 0.0 -> 0.002)\n",
       "sampling: org.apache.spark.sql.DataFrame = [Time: decimal(10,0), V1: double ... 29 more fields]\n"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//mapping the data to balance\n",
    "val fractionKeyMap1= Map(1.0->1.0, 0.0->0.002)\n",
    "//stat.sampleBy used to sample down the data\n",
    "val sampling= creditCard_df.stat.sampleBy(\"Class\",fractionKeyMap1, 111)\n",
    "//.groupBy(\"Class\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.RandomForestClassifier\n",
       "import org.apache.spark.ml.feature._\n",
       "rf1: org.apache.spark.ml.classification.RandomForestClassifier = rfc_8cd412573f9b\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.feature._\n",
    "//create a Random Forest model using RandomForestClassifier in spark to predict the Class for this dataset.\n",
    "val rf1 = new RandomForestClassifier().setLabelCol(\"Class\").setFeaturesCol(\"feature_vector1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_rf1: org.apache.spark.ml.Pipeline = pipeline_133af343a912\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//fit stages in the pipeline\n",
    "val pipeline_rf1 = new Pipeline().setStages(Array(assembler,standardized, assembler1,rf1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Time: decimal(10,0), V1: double ... 29 more fields]\n",
       "testing1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Time: decimal(10,0), V1: double ... 29 more fields]\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//dividing data into trainig and testing set\n",
    "val Array(training1,testing1)=sampling.randomSplit(Array(0.8,0.2),111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipelineModel_rf1: org.apache.spark.ml.PipelineModel = pipeline_133af343a912\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Fit the training data to the pipeline\n",
    "val pipelineModel_rf1 = pipeline_rf1.fit(training1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|Class|         probability|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|    0|[0.85092936668793...|\n",
      "|       1.0|    1|           [0.0,1.0]|\n",
      "|       1.0|    1|           [0.0,1.0]|\n",
      "|       1.0|    1|           [0.0,1.0]|\n",
      "|       1.0|    1|           [0.0,1.0]|\n",
      "|       1.0|    1|           [0.0,1.0]|\n",
      "|       1.0|    1|           [0.0,1.0]|\n",
      "|       0.0|    0|[0.94007073557125...|\n",
      "|       0.0|    0|[0.82676779657402...|\n",
      "|       1.0|    1|           [0.0,1.0]|\n",
      "|       0.0|    0|[0.82632182176102...|\n",
      "|       0.0|    0|[0.89531684103582...|\n",
      "|       1.0|    1|[0.30781249213438...|\n",
      "|       1.0|    1|           [0.0,1.0]|\n",
      "|       1.0|    1|           [0.0,1.0]|\n",
      "|       1.0|    1|           [0.0,1.0]|\n",
      "|       1.0|    1|           [0.0,1.0]|\n",
      "|       1.0|    1|           [0.0,1.0]|\n",
      "|       0.0|    0|[0.95709351339265...|\n",
      "|       1.0|    1|           [0.0,1.0]|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions1: org.apache.spark.sql.DataFrame = [Time: decimal(10,0), V1: double ... 35 more fields]\n"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Make predictions.\n",
    "val predictions1 = pipelineModel_rf1.transform(testing1)\n",
    "predictions1.select(\"prediction\",\"Class\",\"probability\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|Class| scaled_and_centered|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|    0|[-0.3359314193975...|\n",
      "|       1.0|    1|[-0.4748082390912...|\n",
      "|       1.0|    1|[-0.4748082390912...|\n",
      "|       1.0|    1|[-0.4748082390912...|\n",
      "|       1.0|    1|[-0.4748082390912...|\n",
      "|       1.0|    1|[-0.4748082390912...|\n",
      "|       1.0|    1|[-0.4748082390912...|\n",
      "|       0.0|    0|[-0.1238803503823...|\n",
      "|       0.0|    0|[0.12417939072971...|\n",
      "|       1.0|    1|[-0.4748082390912...|\n",
      "|       0.0|    0|[-0.4213977856998...|\n",
      "|       0.0|    0|[-0.4763025748810...|\n",
      "|       1.0|    1|[-0.4613592169827...|\n",
      "|       1.0|    1|[0.43037361450429...|\n",
      "|       1.0|    1|[-0.4748082390912...|\n",
      "|       1.0|    1|[-0.4748082390912...|\n",
      "|       1.0|    1|[0.00236692166826...|\n",
      "|       1.0|    1|[0.00236692166826...|\n",
      "|       0.0|    0|[-0.3272546309404...|\n",
      "|       1.0|    1|[0.00236692166826...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Area under ROC curve(AUC) for RF on test data = 0.9671067352875363\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "paramGrid_rf: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\trfc_75d4828f3d10-maxDepth: 2,\n",
       "\trfc_75d4828f3d10-numTrees: 5\n",
       "}, {\n",
       "\trfc_75d4828f3d10-maxDepth: 2,\n",
       "\trfc_75d4828f3d10-numTrees: 20\n",
       "}, {\n",
       "\trfc_75d4828f3d10-maxDepth: 5,\n",
       "\trfc_75d4828f3d10-numTrees: 5\n",
       "}, {\n",
       "\trfc_75d4828f3d10-maxDepth: 5,\n",
       "\trfc_75d4828f3d10-numTrees: 20\n",
       "})\n",
       "evaluator_rf: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_a80c85f984f8\n",
       "cv_rf: org.apache.spark.ml.tuning.CrossValidator = cv_3a5f63ce1fc8\n",
       "AUC: Double = 0.9671067352875363\n"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    " val paramGrid_rf =new ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, Array(2, 5))\n",
    "             .addGrid(rf.numTrees, Array(5, 20))\n",
    "             .build()\n",
    "\n",
    " val evaluator_rf = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"Class\").setMetricName(\"areaUnderROC\")\n",
    "\n",
    "\n",
    "val cv_rf = new CrossValidator().setEstimator(rf1).setEvaluator(evaluator_rf).setEstimatorParamMaps(paramGrid_rf).setNumFolds(2)\n",
    "\n",
    "\n",
    "\n",
    "val AUC = evaluator.evaluate(predictions1)\n",
    "predictions1.select(\"prediction\",\"Class\",\"scaled_and_centered\").show()\n",
    "println(s\"Area under ROC curve(AUC) for RF on test data = $AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
       "gbt: org.apache.spark.ml.classification.GBTClassifier = gbtc_19a4a8bc5a40\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
    "//create a Gradient Boost Tree model using GBTClassifier in spark to predict the Class for this dataset.\n",
    "val gbt = new GBTClassifier()\n",
    "  .setLabelCol(\"Class\")\n",
    "  .setFeaturesCol(\"feature_vector1\").setMaxIter(10)\n",
    "//default depth of tree is 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_gbt: org.apache.spark.ml.Pipeline = pipeline_9d88eda9d420\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//fitting stages to pipeline\n",
    "val pipeline_gbt = new Pipeline().setStages(Array(assembler,standardized, assembler1,gbt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training_gbt: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Time: decimal(10,0), V1: double ... 29 more fields]\n",
       "testing_gbt: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Time: decimal(10,0), V1: double ... 29 more fields]\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//dividing sampled data in training and testing data\n",
    "val Array(training_gbt,testing_gbt)=sampling.randomSplit(Array(0.8,0.2),111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipelineModel_gbt: org.apache.spark.ml.PipelineModel = pipeline_9d88eda9d420\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipelineModel_gbt = pipeline_gbt.fit(training_gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|Class|         probability|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|    0|[0.92963109708144...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "|       0.0|    0|[0.92659704687853...|\n",
      "|       0.0|    0|[0.90350837796840...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "|       0.0|    0|[0.92759874330279...|\n",
      "|       0.0|    0|[0.75187648812100...|\n",
      "|       1.0|    1|[0.41259766059674...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "|       0.0|    0|[0.91310746078303...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions_gbt: org.apache.spark.sql.DataFrame = [Time: decimal(10,0), V1: double ... 35 more fields]\n"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//making predictions\n",
    "val predictions_gbt = pipelineModel_gbt.transform(testing_gbt)\n",
    "predictions_gbt.select(\"prediction\",\"Class\",\"probability\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9137931034482759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "evaluator_gbt: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_a159df9080de\n",
       "Accuracy: Double = 0.9137931034482759\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "val evaluator_gbt= new MulticlassClassificationEvaluator()\n",
    ".setLabelCol(\"Class\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\n",
    "val Accuracy= evaluator_gbt.evaluate(predictions_gbt)\n",
    "println(s\"Accuracy=$Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml._\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_0ae26448c1c6\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml._\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "//using logistc regression again to test sampled data By sampleBy method \n",
    "val lr = new LogisticRegression().setLabelCol(\"Class\").setFeaturesCol(\"feature_vector1\")\n",
    ".setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_lr: org.apache.spark.ml.Pipeline = pipeline_604b4ec240df\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline_lr = new Pipeline().setStages(Array(assembler,standardized, assembler1,gbt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training_lr: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Time: decimal(10,0), V1: double ... 29 more fields]\n",
       "testing_lr: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Time: decimal(10,0), V1: double ... 29 more fields]\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training_lr,testing_lr)=sampling.randomSplit(Array(0.8,0.2),111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipelineModel_lr: org.apache.spark.ml.PipelineModel = pipeline_604b4ec240df\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipelineModel_lr = pipeline_lr.fit(training_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|Class|         probability|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|    0|[0.92963109708144...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "|       0.0|    0|[0.92659704687853...|\n",
      "|       0.0|    0|[0.90350837796840...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "|       0.0|    0|[0.92759874330279...|\n",
      "|       0.0|    0|[0.75187648812100...|\n",
      "|       1.0|    1|[0.41259766059674...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "|       0.0|    0|[0.91310746078303...|\n",
      "|       1.0|    1|[0.06588111931387...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions_lr: org.apache.spark.sql.DataFrame = [Time: decimal(10,0), V1: double ... 35 more fields]\n"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions_lr = pipelineModel_lr.transform(testing_lr)\n",
    "predictions_lr.select(\"prediction\",\"Class\",\"probability\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|Class|         probability|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|    0|[0.95263376097556...|\n",
      "|       1.0|    1|[2.00993105235670...|\n",
      "|       1.0|    1|[9.54629059496377...|\n",
      "|       1.0|    1|[1.12611289681051...|\n",
      "|       1.0|    1|[7.16736636931522...|\n",
      "|       1.0|    1|[0.00113885522413...|\n",
      "|       1.0|    1|[1.05538180094523...|\n",
      "|       0.0|    0|[0.92817784652995...|\n",
      "|       0.0|    0|[0.80393036260579...|\n",
      "|       1.0|    1|[2.90908992895979...|\n",
      "|       0.0|    0|[0.95410443842753...|\n",
      "|       0.0|    0|[0.97553884320673...|\n",
      "|       0.0|    1|[0.65411445829338...|\n",
      "|       1.0|    1|[3.16742815844637...|\n",
      "|       1.0|    1|[1.14056625858585...|\n",
      "|       1.0|    1|[1.28766568459304...|\n",
      "|       1.0|    1|[2.73149858061735...|\n",
      "|       1.0|    1|[7.23626083217651...|\n",
      "|       0.0|    0|[0.93282920287423...|\n",
      "|       1.0|    1|[1.84255942494730...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Area under ROC curve(AUC) for LR on test data = 0.9553964346982919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml._\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "lr1: org.apache.spark.ml.classification.LogisticRegression = logreg_aa5be670a6c1\n",
       "paramGrid1: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlogreg_aa5be670a6c1-elasticNetParam: 0.0,\n",
       "\tlogreg_aa5be670a6c1-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_aa5be670a6c1-elasticNetParam: 0.0,\n",
       "\tlogreg_aa5be670a6c1-regParam: 0.5\n",
       "}, {\n",
       "\tlogreg_aa5be670a6c1-elasticNetParam: 0.0,\n",
       "\tlogreg_aa5be670a6c1-regParam: 2.0\n",
       "}, {\n",
       "\tlogreg_aa5be670a6c1-elasticNetParam: 0.5,\n",
       "\tlogreg_aa5be670a6c1-regParam: 0.01\n",
       "}, {\n",
       "\tlogreg_aa5be670a6c1-elasticNetParam: 0.5,\n",
       "\tlogreg_aa5be670a6c1-regParam: 0.5\n",
       "}, {\n",
       "\tlogreg_aa5be670a6c1-elasticNetParam: 0.5,\n",
       "\tlogreg_aa5be670a6c1-regParam: 2.0\n",
       "}, {\n",
       "\tlogreg_aa5be670a6c1-elasticNetParam: 1.0,\n",
       "\tlogreg_aa5be670a6c..."
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml._\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "val lr1 = new LogisticRegression().setLabelCol(\"Class\").setFeaturesCol(\"feature_vector1\")\n",
    ".setMaxIter(10)\n",
    "val paramGrid1 =new ParamGridBuilder()\n",
    "             .addGrid(lr1.regParam, Array(0.01,0.5,2.0))\n",
    "             .addGrid(lr1.elasticNetParam, Array(0.0,0.5,1.0))\n",
    "              //.addGrid(tfidf.minDocFreq, Array(5,10))//tfidf not done, dow we need this\n",
    "             .build()\n",
    "\n",
    "val evaluator_lr1 = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"Class\").setMetricName(\"areaUnderROC\")\n",
    "\n",
    "\n",
    "val cv_lr = new CrossValidator().setEstimator(lr1).setEvaluator(evaluator_lr1).setEstimatorParamMaps(paramGrid1).setNumFolds(2)\n",
    "\n",
    "val pipeline_lr1 = new Pipeline().setStages(Array(assembler,standardized, assembler1,cv_lr))//is pipeline ok\n",
    "\n",
    "val Array(training_lr1,testing_lr1)=sampling.randomSplit(Array(0.8,0.2),111)\n",
    "\n",
    "//Fit the training data to the pipeline\n",
    "val pipelineModel_lr1 = pipeline_lr1.fit(training_lr1)\n",
    "\n",
    "// Make predictions.\n",
    "val predictions_lr1 = pipelineModel_lr1.transform(testing_lr1)\n",
    "val AUC_lr1 = evaluator_lr1.evaluate(predictions_lr1)\n",
    "predictions_lr1.select(\"prediction\",\"Class\",\"probability\").show()\n",
    "println(s\"Area under ROC curve(AUC) for LR on test data = $AUC_lr1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
